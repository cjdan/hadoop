1.基本架构及原理
    Apache Spark是一个围绕速度、易用性和复杂分析构建的大数据处理框架, 最初在2009年由加州大学伯克利分校的AMPLab开发, 并于2010年成为Apache的
    开源项目之一, 与Hadoop和Storm等其他大数据和MapReduce技术相比, Spark有如下优势：
        1) Spark提供了一个全面、统一的框架用于管理各种有着不同性质(文本数据、图表数据等)的数据集和数据源(批量数据或实时的流数据)的大数据处理的需求;
        2) 官方资料介绍Spark可以将Hadoop集群中的应用在内存中的运行速度提升100倍, 甚至能够将应用在磁盘上的运行速度提升10倍。
2.架构及生态
    通常当需要处理的数据量超过了单机尺度(比如我们的计算机有4GB的内存, 而我们需要处理100GB以上的数据)这时我们可以选择spark集群进行计算, 有时
    我们可能需要处理的数据量并不大, 但是计算很复杂, 需要大量的时间, 这时我们也可以选择利用spark集群强大的计算资源, 并行化地计算, 其架构示意图如
    Spark架构图.png所示。
    名词解释包括如下：
    Spark Core：包含Spark的基本功能；尤其是定义RDD的API、操作以及这两者上的动作。其他Spark的库都是构建在RDD和Spark Core之上的；
    Spark SQL：提供通过Apache Hive的SQL变体Hive查询语言(HiveQL)与Spark进行交互的API。每个数据库表被当做一个RDD, 
               Spark SQL查询被转换为Spark操作；
    Spark Streaming：对实时数据流进行处理和控制。Spark Streaming允许程序能够像普通RDD一样处理实时数据；
    Mlib：一个常用机器学习算法库, 算法被实现为对RDD的Spark操作。这个库包含可扩展的学习算法, 比如分类、回归等需要对大量数据集进行迭代的操作；
    GraphX：控制图、并行图操作和计算的一组算法和工具的集合。GraphX扩展了RDD API, 包含控制图、创建子图、访问路径上所有顶点的操作；
    Spark架构的组成图如Spark架构的组成图.png所示
    Cluster Manager：在standalone模式中即为Master主节点, 控制整个集群, 监控worker。在YARN模式中为资源管理器；
    Worker节点：从节点, 负责控制计算节点, 启动Executor或者Driver；
    Driver： 运行Application 的main()函数；
    Executor：执行器, 是为某个Application运行在worker node上的一个进程；
3.Spark与hadoop的关系
    Hadoop有两个核心模块, 分布式存储模块HDFS和分布式计算模块Mapreduce
    spark本身并没有提供分布式文件系统, 因此spark的分析大多依赖于Hadoop的分布式文件系统HDFS
    Hadoop的Mapreduce与spark都可以进行数据计算, 而相比于Mapreduce, spark的速度更快并且提供的功能更加丰富
    Spark与hadoop的关系如图Spark与hadoop.png
4.运行流程及特点
    4.1 spark运行流程图如Spark运行流程图.png所示
        1) 构建Spark Application的运行环境, 启动SparkContext;
        2) SparkContext向资源管理器（可以是Standalone, Mesos, Yarn）申请运行Executor资源, 并启动StandaloneExecutorbackend;
        3) Executor向SparkContext申请Task;
        4) SparkContext将应用程序分发给Executor;
        5) SparkContext构建成DAG图, 将DAG图分解成Stage、将Taskset发送给Task Scheduler, 最后由Task Scheduler将Task发送给Executor运行;
        6) Task在Executor上运行, 运行完释放所有资源。
    4.2 Spark运行特点
        1) 每个Application获取专属的executor进程, 该进程在Application期间一直驻留, 并以多线程方式运行Task。
            这种Application隔离机制是有优势的, 无论是从调度角度看（每个Driver调度他自己的任务）, 还是从运行角度
            看（来自不同Application的Task运行在不同JVM中）, 当然这样意味着Spark Application不能跨应用程序共享数据,
            除非将数据写入外部存储系统;
        2) Spark与资源管理器无关, 只要能够获取executor进程, 并能保持相互通信就可以了;
        3) 提交SparkContext的Client应该靠近Worker节点（运行Executor的节点）, 最好是在同一个Rack里, 因为
            Spark Application运行过程中SparkContext和Executor之间有大量的信息交换;
        4) Task采用了数据本地性和推测执行的优化机制。
5.常用术语
    Application: Appliction都是指用户编写的Spark应用程序, 其中包括一个Driver功能的代码和分布在集群中多个节点上运行的Executor代码
    Driver:  Spark中的Driver即运行上述Application的main函数并创建SparkContext, 创建SparkContext的目的是为了准备
        Spark应用程序的运行环境, 在Spark中有SparkContext负责与ClusterManager通信, 进行资源申请、任务的分配和监控等, 
        当Executor部分运行完毕后, Driver同时负责将SparkContext关闭, 通常用SparkContext代表Driver
    Executor:  某个Application运行在worker节点上的一个进程,   该进程负责运行某些Task,  并且负责将数据存到内存或磁盘上, 
        每个Application都有各自独立的一批Executor,  在Spark on Yarn模式下, 其进程名称为CoarseGrainedExecutor Backend。
        一个CoarseGrainedExecutor Backend有且仅有一个Executor对象,  负责将Task包装成taskRunner,并从线程池中抽取一个空闲
        线程运行Task,  这个每一个oarseGrainedExecutor Backend能并行运行Task的数量取决与分配给它的cpu个数
    Cluter Manager：指的是在集群上获取资源的外部服务。目前有三种类型,如下
        Standalon : spark原生的资源管理, 由Master负责资源的分配
        Apache Mesos:与hadoop MR兼容性良好的一种资源调度框架
        Hadoop Yarn: 主要是指Yarn中的ResourceManager
    Worker: 集群中任何可以运行Application代码的节点, 在Standalone模式中指的是通过slave文件配置的Worker节点, 在
        Spark on Yarn模式下就是NoteManager节点
    Task: 被送到某个Executor上的工作单元, 但hadoopMR中的MapTask和ReduceTask概念一样, 是运行Application的基本单位, 
        多个Task组成一个Stage, 而Task的调度和管理等是由TaskScheduler负责
    Job: 包含多个Task组成的并行计算, 往往由Spark Action触发生成,  一个Application中往往会产生多个Job
    Stage: 每个Job会被拆分成多组Task,  作为一个TaskSet,  其名称为Stage, Stage的划分和调度是有DAGScheduler来负责的, 
        Stage有非最终的Stage（Shuffle Map Stage）和最终的Stage（Result Stage）两种, Stage的边界就是发生shuffle的地方
    DAGScheduler: 根据Job构建基于Stage的DAG（Directed Acyclic Graph有向无环图), 并提交Stage给TASkScheduler。
        其划分Stage的依据是RDD之间的依赖的关系找出开销最小的调度方法, 如DAGScheduler.png所示
    TASKSedulter: 将TaskSET提交给worker运行, 每个Executor运行什么Task就是在此处分配的. TaskScheduler维护所有TaskSet, 
        当Executor向Driver发生心跳时, TaskScheduler会根据资源剩余情况分配相应的Task。另外TaskScheduler还维护着所有Task
        的运行标签, 重试失败的Task。TaskScheduler的作用如TaskScheduler.png
    在不同运行模式中任务调度器具体为：
        Spark on Standalone模式为TaskScheduler
        YARN-Client模式为YarnClientClusterScheduler
        YARN-Cluster模式为YarnClusterScheduler
    总体示意如总体层次示意图.png示意。
    Job=多个stage, Stage=多个同种task, Task分为ShuffleMapTask和ResultTask, Dependency分为ShuffleDependency和NarrowDependency
6.Spark运行模式
    Spark的运行模式多种多样，灵活多变，部署在单机上时，既可以用本地模式运行，也可以用伪分布模式运行，而当以分布式集群的方式部署时，也有众多的运行
    模式可供选择，这取决于集群的实际情况，底层的资源调度即可以依赖外部资源调度框架，也可以使用Spark内建的Standalone模式。
    对于外部资源调度框架的支持，目前的实现包括相对稳定的Mesos模式，以及hadoop YARN模式
    本地模式：常用于本地开发测试，本地还分别 local 和 local cluster
    6.1 standalone: 独立集群运行模式
        Standalone模式使用Spark自带的资源调度框架。
        采用Master/Slaves的典型架构，选用ZooKeeper来实现Master的HA。
        框架结构图如standalone.png所示。
        该模式主要的节点有Client节点、Master节点和Worker节点。其中Driver既可以运行在Master节点上中，也可以运行在本地Client端。当用
        spark-shell交互式工具提交Spark的Job时，Driver在Master节点上运行；当使用spark-submit工具提交Job或者在Eclips、IDEA等开发
        平台上使用”new SparkConf.setManager(“spark://master:7077”)”方式运行Spark任务时，Driver是运行在本地Client端上的。
        运行过程如standalone架构图.png所示
        1) SparkContext连接到Master，向Master注册并申请资源（CPU Core 和Memory）
        2) Master根据SparkContext的资源申请要求和Worker心跳周期内报告的信息决定在哪个Worker上分配资源，然后在该Worker上获取资源，
        然后启动StandaloneExecutorBackend；
        3) StandaloneExecutorBackend向SparkContext注册；
        4) SparkContext将Applicaiton代码发送给StandaloneExecutorBackend；并且SparkContext解析Applicaiton代码，构建DAG图，
        并提交给DAG Scheduler分解成Stage（当碰到Action操作时，就会催生Job；每个Job中含有1个或多个Stage，Stage一般在获取外部数
        据和shuffle之前产生），然后以Stage（或者称为TaskSet）提交给Task Scheduler，Task Scheduler负责将Task分配到相应的
        Worker，最后提交给StandaloneExecutorBackend执行；
        5) StandaloneExecutorBackend会建立Executor线程池，开始执行Task，并向SparkContext报告，直至Task完成;
        6) 所有Task完成后，SparkContext向Master注销，释放资源。
    6.2 yarn:
        spark on YARN模式根据Driver在集群中的位置分为两种模式：一种是YARN-Client模式，另一种是YARN-Cluster（或称为YARN-Standalone模式）
        Yarn-Client模式中，Driver在客户端本地运行，这种模式可以使得Spark Application和客户端进行交互，因为Driver在客户端，所以可以通过
        webUI访问Driver的状态，默认是http://master:4040访问，而YARN通过http:// master:8088访问
        YARN-client：
            YARN-client的工作流程步骤如YARN-client工作流程.png所示。
            1) Spark Yarn Client向YARN的ResourceManager申请启动Application Master。同时在SparkContent初始化中将创建DAGScheduler
            和TASKScheduler等，由于我们选择的是Yarn-Client模式，程序会选择YarnClientClusterScheduler和YarnClientSchedulerBackend;
            2) ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应
            用程序的ApplicationMaster，与YARN-Cluster区别的是在该ApplicationMaster不运行SparkContext，只与SparkContext进行联系进
            行资源的分派;
            3) Client中的SparkContext初始化完毕后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向
            ResourceManager申请资源（Container）;
            4) 一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动
            CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向Client中的SparkContext注册并申请Task;
            5) client中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向
            Driver汇报运行的状态和进度，以让Client随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务;
            6) 应用程序运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己。
        Spark Cluster模式:
            在YARN-Cluster模式中，当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：
                第一个阶段是把Spark的Driver作为一个ApplicationMaster在YARN集群中先启动；
                第二个阶段是由ApplicationMaster创建应用程序，然后为它向ResourceManager申请资源，并启动Executor来运行Task，同时监控它
                的整个运行过程，直到运行完成。
            YARN-cluster的工作流程分为以下几个步骤，如图YARN-cluster工作流程.png所示
                1) Spark Yarn Client向YARN中提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在
                Executor中运行的程序等;
                2) ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container
                中启动应用程序的ApplicationMaster，其中ApplicationMaster进行SparkContext等的初始化
                3) ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将采用
                轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束
                4) 一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动
                CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册
                并申请Task。这一点和Standalone模式一样，只不过SparkContext在Spark Application中初始化时，使用
                CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对
                TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑等
                5) ApplicationMaster中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend
                运行Task并向ApplicationMaster汇报运行的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败
                时重新启动任务
                6) 应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己
            Spark Client 和 Spark Cluster的区别:
                理解YARN-Client和YARN-Cluster深层次的区别之前先清楚一个概念：Application Master。在YARN中，每个Application实例都有
                一个ApplicationMaster进程，它是Application启动的第一个容器。它负责和ResourceManager打交道并请求资源，获取资源之后告诉
                NodeManager为其启动Container。从深层次的含义讲YARN-Cluster和YARN-Client模式的区别其实就是ApplicationMaster进程的区别。
                YARN-Cluster模式下，Driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作
                业之后，就可以关掉Client，作业会继续在YARN上运行，因而YARN-Cluster模式不适合运行交互类型的作业。
                YARN-Client模式下，Application Master仅仅向YARN请求Executor，Client会和请求的Container通信来调度他们工作，也就是
                说Client不能离开。

7.SparkCore
    7.1 RDD
        RDD(Resilient Distributed Dateset)，弹性分布式数据集。
    7.2 RDD的五大特性
        1) RDD是由一系列的partition组成的。
        2) 函数是作用在每一个partition（split）上的。
        3) RDD之间有一系列的依赖关系。
        4) 分区器是作用在K,V格式的RDD上。
        5) RDD提供一系列最佳的计算位置。
    7.3 RDD理解图
        参照RDD示意图.png
        注意：
            textFile方法底层封装的是读取MR读取文件的方式，读取文件之前先split，默认split大小是一个block大小。
            RDD实际上不存储数据，这里方便理解，暂时理解为存储数据。
            K,V格式的RDD:如果RDD里面存储的数据都是二元组对象，那么这个RDD我们就叫做K,V格式的RDD。
            RDD的弹性（容错）:
                	partition数量，大小没有限制，体现了RDD的弹性
                	RDD之间依赖关系，可以基于上一个RDD重新计算出RDD。
            RDD的分布式: RDD是由Partition组成，partition是分布在不同节点上的。
            RDD提供计算最佳位置，体现了数据本地化。体现了大数据中“计算移动数据不移动”的理念。

    7.4 RDD运行流程
           RDD在Spark中运行大概分为以下三步：
               1) 创建RDD对象;
               2) DAGScheduler模块介入运算，计算RDD之间的依赖关系，RDD之间的依赖关系就形成了DAG;
               3) 每一个Job被分为多个Stage。划分Stage的一个主要依据是当前计算因子的输入是否是确定的，如果是则将其分在同一个Stage，避免多
               个Stage之间的消息传递开销。
           示例图参考RDD运行流程图.png
8.Spark代码流程
    8.1 创建SparkConf对象
           设置Application name。
        	设置运行模式及资源需求。
    8.2 创建SparkContext对象。
    8.3 基于Spark的上下文创建一个RDD，对RDD进行处理。
    8.4 应用程序中要有Action类算子来触发Transformation类算子执行。
    8.5 关闭Spark上下文对象SparkContext。
9.Transformations转换算子
    Transformations类算子是一类算子（函数）叫做转换算子，如map,flatMap,reduceByKey等。Transformations算子是延迟执行，也叫懒加载。
    Transformation类算子如下：
        	filter：过滤符合条件的记录数，true保留，false过滤掉。
        	map：将一个RDD中的每个数据项，通过map中的函数映射变为一个新的元素。特点：输入一条，输出一条数据。
        	mapPartitionWithIndex：类似于mapPartitions,除此之外还会携带分区的索引值。
        	repartition：增加或减少分区。会产生shuffle。（多个分区分到一个分区不会产生shuffle）
        	coalesce：coalesce常用来减少分区，第二个参数是减少分区的过程中是否产生shuffle。
                     true为产生shuffle，false不产生shuffle。默认是false。
                     如果coalesce设置的分区数比原来的RDD的分区数还多的话，第二个参数设置为false不会起作用，如果设置成true，
                     效果和repartition一样。即repartition(numPartitions) = coalesce(numPartitions,true)。
        	groupByKey：作用在K，V格式的RDD上。根据Key进行分组。作用在（K，V），返回（K，Iterable <V>）。
        	zip：将两个RDD中的元素（KV格式/非KV格式）变成一个KV格式的RDD,两个RDD的每个分区元素个数必须相同。
        	zipWithIndex：该函数将RDD中的元素和这个元素在RDD中的索引号（从0开始）组合成（K,V）对。
        	flatMap：先map后flat。与map类似，每个输入项可以映射为0到多个输出项。
        	sample：随机抽样算子，根据传进去的小数按比例进行又放回或者无放回的抽样。
        	reduceByKey：将相同的Key根据相应的逻辑进行处理。
        	sortByKey/sortBy：作用在K,V格式的RDD上，对key进行升序或者降序排序。
        	repartitionAndSortWithinPartitions：根据给定的分区程序对RDD进行重新分区，并在每个生成的分区内按键对记录进行排序。
            这比调用重新分区，然后在每个分区内进行排序更有效率，因为它可以将排序压入洗牌机器。
            主要用途：1）如果需要重分区，并且想要对分区中的数据进行升序排序。
                    2）提高性能，替换repartition和sortBy。
        	combineByKey:createCombiner,这个函数把当前的值作为参数，此时我们可以对其做些附加操作(类型转换)并把它返回 (这一步类似于初始化操作)
                         mergeValue,该函数把元素V合并到之前的元素C(createCombiner)上 (这个操作在每个分区内进行)
                         mergeCombiners,该函数把2个元素C合并 (这个操作在不同分区间进行)。
10.Action行动算子
    Action类算子也是一类算子（函数）叫做行动算子，如foreach,collect，count等。Transformations类算子是延迟执行，Action类算子是触发执行。
    一个application应用程序中有几个Action类算子执行，就有几个job运行。
    Action类算子如下：
        	count：返回数据集中的元素数。会在结果计算完成后回收到Driver端。
        	countByKey:作用到K,V格式的RDD上，根据Key计数相同Key的数据集元素。
        	countByValue:根据数据集每个元素相同的内容来计数。返回相同内容的元素对应的条数。
        	reduce:根据聚合逻辑聚合数据集中的每个元素。
        	take(n)：返回一个包含数据集前n个元素的集合。
        	first:first=take(1),返回数据集中的第一个元素。
        	foreach:循环遍历数据集中的每个元素，运行相应的逻辑。
        	collect：将计算结果回收到Driver端。
11.控制算子
    控制算子有三种，cache,persist,checkpoint，以上算子都可以将RDD持久化，持久化的单位是partition。cache和persist都是懒执行的。
    必须有一个action类算子触发执行。checkpoint算子不仅能将RDD持久化到磁盘，还能切断RDD之间的依赖关系。
    cache：默认将RDD的数据持久化到内存中。cache是懒执行。
    persist：可以指定持久化的级别。最常用的是MEMORY_ONLY和MEMORY_AND_DISK。”_2”表示有副本数。
        cache和persist的注意事项：
        1) cache和persist都是懒执行，必须有一个action类算子触发执行。
        2) cache和persist算子的返回值可以赋值给一个变量，在其他job中直接使用这个变量就是使用持久化的数据了。持久化的单位是partition。
        3) cache和persist算子后不能立即紧跟action算子。
            错误：rdd.cache().count() 返回的不是持久化的RDD，而是一个数值了。
    checkpoint:checkpoint将RDD持久化到磁盘，还可以切断RDD之间的依赖关系。
    checkpoint 的执行原理：
        1) 当RDD的job执行完毕后，会从finalRDD从后往前回溯。
        2) 当回溯到某一个RDD调用了checkpoint方法，会对当前的RDD做一个标记。
        3) Spark框架会自动启动一个新的job，重新计算这个RDD的数据，将数据持久化到HDFS上。
        优化：对RDD执行checkpoint之前，最好对这个RDD先执行cache，这样新启动的job只需要将内存中的数据拷贝到HDFS上就可以，省去了重新计算这一步。










