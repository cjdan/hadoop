1.基本架构及原理
    Apache Spark是一个围绕速度、易用性和复杂分析构建的大数据处理框架, 最初在2009年由加州大学伯克利分校的AMPLab开发, 并于2010年成为Apache的
    开源项目之一, 与Hadoop和Storm等其他大数据和MapReduce技术相比, Spark有如下优势：
        1) Spark提供了一个全面、统一的框架用于管理各种有着不同性质(文本数据、图表数据等)的数据集和数据源(批量数据或实时的流数据)的大数据处理的需求;
        2) 官方资料介绍Spark可以将Hadoop集群中的应用在内存中的运行速度提升100倍, 甚至能够将应用在磁盘上的运行速度提升10倍。
2.架构及生态
    通常当需要处理的数据量超过了单机尺度(比如我们的计算机有4GB的内存, 而我们需要处理100GB以上的数据)这时我们可以选择spark集群进行计算, 有时
    我们可能需要处理的数据量并不大, 但是计算很复杂, 需要大量的时间, 这时我们也可以选择利用spark集群强大的计算资源, 并行化地计算, 其架构示意图如
    Spark架构图.png所示。
    名词解释包括如下：
    Spark Core：包含Spark的基本功能；尤其是定义RDD的API、操作以及这两者上的动作。其他Spark的库都是构建在RDD和Spark Core之上的；
    Spark SQL：提供通过Apache Hive的SQL变体Hive查询语言(HiveQL)与Spark进行交互的API。每个数据库表被当做一个RDD, 
               Spark SQL查询被转换为Spark操作；
    Spark Streaming：对实时数据流进行处理和控制。Spark Streaming允许程序能够像普通RDD一样处理实时数据；
    Mlib：一个常用机器学习算法库, 算法被实现为对RDD的Spark操作。这个库包含可扩展的学习算法, 比如分类、回归等需要对大量数据集进行迭代的操作；
    GraphX：控制图、并行图操作和计算的一组算法和工具的集合。GraphX扩展了RDD API, 包含控制图、创建子图、访问路径上所有顶点的操作；
    Spark架构的组成图如Spark架构的组成图.png所示
    Cluster Manager：在standalone模式中即为Master主节点, 控制整个集群, 监控worker。在YARN模式中为资源管理器；
    Worker节点：从节点, 负责控制计算节点, 启动Executor或者Driver；
    Driver： 运行Application 的main()函数；
    Executor：执行器, 是为某个Application运行在worker node上的一个进程；
3.Spark与hadoop的关系
    Hadoop有两个核心模块, 分布式存储模块HDFS和分布式计算模块Mapreduce
    spark本身并没有提供分布式文件系统, 因此spark的分析大多依赖于Hadoop的分布式文件系统HDFS
    Hadoop的Mapreduce与spark都可以进行数据计算, 而相比于Mapreduce, spark的速度更快并且提供的功能更加丰富
    Spark与hadoop的关系如图Spark与hadoop.png
4.运行流程及特点
    4.1 spark运行流程图如Spark运行流程图.png所示
        1) 构建Spark Application的运行环境, 启动SparkContext;
        2) SparkContext向资源管理器（可以是Standalone, Mesos, Yarn）申请运行Executor资源, 并启动StandaloneExecutorbackend;
        3) Executor向SparkContext申请Task;
        4) SparkContext将应用程序分发给Executor;
        5) SparkContext构建成DAG图, 将DAG图分解成Stage、将Taskset发送给Task Scheduler, 最后由Task Scheduler将Task发送给Executor运行;
        6) Task在Executor上运行, 运行完释放所有资源。
    4.2 Spark运行特点
        1) 每个Application获取专属的executor进程, 该进程在Application期间一直驻留, 并以多线程方式运行Task。
            这种Application隔离机制是有优势的, 无论是从调度角度看（每个Driver调度他自己的任务）, 还是从运行角度
            看（来自不同Application的Task运行在不同JVM中）, 当然这样意味着Spark Application不能跨应用程序共享数据,
            除非将数据写入外部存储系统;
        2) Spark与资源管理器无关, 只要能够获取executor进程, 并能保持相互通信就可以了;
        3) 提交SparkContext的Client应该靠近Worker节点（运行Executor的节点）, 最好是在同一个Rack里, 因为
            Spark Application运行过程中SparkContext和Executor之间有大量的信息交换;
        4) Task采用了数据本地性和推测执行的优化机制。
5.常用术语
    Application: Appliction都是指用户编写的Spark应用程序, 其中包括一个Driver功能的代码和分布在集群中多个节点上运行的Executor代码
    Driver:  Spark中的Driver即运行上述Application的main函数并创建SparkContext, 创建SparkContext的目的是为了准备
        Spark应用程序的运行环境, 在Spark中有SparkContext负责与ClusterManager通信, 进行资源申请、任务的分配和监控等, 
        当Executor部分运行完毕后, Driver同时负责将SparkContext关闭, 通常用SparkContext代表Driver
    Executor:  某个Application运行在worker节点上的一个进程,   该进程负责运行某些Task,  并且负责将数据存到内存或磁盘上, 
        每个Application都有各自独立的一批Executor,  在Spark on Yarn模式下, 其进程名称为CoarseGrainedExecutor Backend。
        一个CoarseGrainedExecutor Backend有且仅有一个Executor对象,  负责将Task包装成taskRunner,并从线程池中抽取一个空闲
        线程运行Task,  这个每一个oarseGrainedExecutor Backend能并行运行Task的数量取决与分配给它的cpu个数
    Cluter Manager：指的是在集群上获取资源的外部服务。目前有三种类型,如下
        Standalon : spark原生的资源管理, 由Master负责资源的分配
        Apache Mesos:与hadoop MR兼容性良好的一种资源调度框架
        Hadoop Yarn: 主要是指Yarn中的ResourceManager
    Worker: 集群中任何可以运行Application代码的节点, 在Standalone模式中指的是通过slave文件配置的Worker节点, 在
        Spark on Yarn模式下就是NoteManager节点
    Task: 被送到某个Executor上的工作单元, 但hadoopMR中的MapTask和ReduceTask概念一样, 是运行Application的基本单位, 
        多个Task组成一个Stage, 而Task的调度和管理等是由TaskScheduler负责
    Job: 包含多个Task组成的并行计算, 往往由Spark Action触发生成,  一个Application中往往会产生多个Job
    Stage: 每个Job会被拆分成多组Task,  作为一个TaskSet,  其名称为Stage, Stage的划分和调度是有DAGScheduler来负责的, 
        Stage有非最终的Stage（Shuffle Map Stage）和最终的Stage（Result Stage）两种, Stage的边界就是发生shuffle的地方
    DAGScheduler: 根据Job构建基于Stage的DAG（Directed Acyclic Graph有向无环图), 并提交Stage给TASkScheduler。
        其划分Stage的依据是RDD之间的依赖的关系找出开销最小的调度方法, 如DAGScheduler.png所示
    TASKSedulter: 将TaskSET提交给worker运行, 每个Executor运行什么Task就是在此处分配的. TaskScheduler维护所有TaskSet, 
        当Executor向Driver发生心跳时, TaskScheduler会根据资源剩余情况分配相应的Task。另外TaskScheduler还维护着所有Task
        的运行标签, 重试失败的Task。TaskScheduler的作用如TaskScheduler.png
    在不同运行模式中任务调度器具体为：
        Spark on Standalone模式为TaskScheduler
        YARN-Client模式为YarnClientClusterScheduler
        YARN-Cluster模式为YarnClusterScheduler
    总体示意如总体层次示意图.png示意。
    Job=多个stage, Stage=多个同种task, Task分为ShuffleMapTask和ResultTask, Dependency分为ShuffleDependency和NarrowDependency
6.Spark运行模式
    Spark的运行模式多种多样，灵活多变，部署在单机上时，既可以用本地模式运行，也可以用伪分布模式运行，而当以分布式集群的方式部署时，也有众多的运行
    模式可供选择，这取决于集群的实际情况，底层的资源调度即可以依赖外部资源调度框架，也可以使用Spark内建的Standalone模式。
    对于外部资源调度框架的支持，目前的实现包括相对稳定的Mesos模式，以及hadoop YARN模式
    本地模式：常用于本地开发测试，本地还分别 local 和 local cluster
    6.1 standalone: 独立集群运行模式
        Standalone模式使用Spark自带的资源调度框架。
        采用Master/Slaves的典型架构，选用ZooKeeper来实现Master的HA。
        框架结构图如standalone.png所示。
        该模式主要的节点有Client节点、Master节点和Worker节点。其中Driver既可以运行在Master节点上中，也可以运行在本地Client端。当用
        spark-shell交互式工具提交Spark的Job时，Driver在Master节点上运行；当使用spark-submit工具提交Job或者在Eclips、IDEA等开发
        平台上使用”new SparkConf.setManager(“spark://master:7077”)”方式运行Spark任务时，Driver是运行在本地Client端上的。
        运行过程如standalone架构图.png所示
        1) SparkContext连接到Master，向Master注册并申请资源（CPU Core 和Memory）
        2) Master根据SparkContext的资源申请要求和Worker心跳周期内报告的信息决定在哪个Worker上分配资源，然后在该Worker上获取资源，
        然后启动StandaloneExecutorBackend；
        3) StandaloneExecutorBackend向SparkContext注册；
        4) SparkContext将Applicaiton代码发送给StandaloneExecutorBackend；并且SparkContext解析Applicaiton代码，构建DAG图，
        并提交给DAG Scheduler分解成Stage（当碰到Action操作时，就会催生Job；每个Job中含有1个或多个Stage，Stage一般在获取外部数
        据和shuffle之前产生），然后以Stage（或者称为TaskSet）提交给Task Scheduler，Task Scheduler负责将Task分配到相应的
        Worker，最后提交给StandaloneExecutorBackend执行；
        5) StandaloneExecutorBackend会建立Executor线程池，开始执行Task，并向SparkContext报告，直至Task完成;
        6) 所有Task完成后，SparkContext向Master注销，释放资源。
    6.2 yarn:
        spark on YARN模式根据Driver在集群中的位置分为两种模式：一种是YARN-Client模式，另一种是YARN-Cluster（或称为YARN-Standalone模式）
        Yarn-Client模式中，Driver在客户端本地运行，这种模式可以使得Spark Application和客户端进行交互，因为Driver在客户端，所以可以通过
        webUI访问Driver的状态，默认是http://master:4040访问，而YARN通过http:// master:8088访问
        YARN-client：
            YARN-client的工作流程步骤如YARN-client工作流程.png所示。
            1) Spark Yarn Client向YARN的ResourceManager申请启动Application Master。同时在SparkContent初始化中将创建DAGScheduler
            和TASKScheduler等，由于我们选择的是Yarn-Client模式，程序会选择YarnClientClusterScheduler和YarnClientSchedulerBackend;
            2) ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应
            用程序的ApplicationMaster，与YARN-Cluster区别的是在该ApplicationMaster不运行SparkContext，只与SparkContext进行联系进
            行资源的分派;
            3) Client中的SparkContext初始化完毕后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向
            ResourceManager申请资源（Container）;
            4) 一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动
            CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向Client中的SparkContext注册并申请Task;
            5) client中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向
            Driver汇报运行的状态和进度，以让Client随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务;
            6) 应用程序运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己。
        Spark Cluster模式:
            在YARN-Cluster模式中，当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：
                第一个阶段是把Spark的Driver作为一个ApplicationMaster在YARN集群中先启动；
                第二个阶段是由ApplicationMaster创建应用程序，然后为它向ResourceManager申请资源，并启动Executor来运行Task，同时监控它
                的整个运行过程，直到运行完成。
            YARN-cluster的工作流程分为以下几个步骤，如图YARN-cluster工作流程.png所示
                1) Spark Yarn Client向YARN中提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在
                Executor中运行的程序等;
                2) ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container
                中启动应用程序的ApplicationMaster，其中ApplicationMaster进行SparkContext等的初始化
                3) ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将采用
                轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束
                4) 一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动
                CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册
                并申请Task。这一点和Standalone模式一样，只不过SparkContext在Spark Application中初始化时，使用
                CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对
                TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑等
                5) ApplicationMaster中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend
                运行Task并向ApplicationMaster汇报运行的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败
                时重新启动任务
                6) 应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己
            Spark Client 和 Spark Cluster的区别:
                理解YARN-Client和YARN-Cluster深层次的区别之前先清楚一个概念：Application Master。在YARN中，每个Application实例都有
                一个ApplicationMaster进程，它是Application启动的第一个容器。它负责和ResourceManager打交道并请求资源，获取资源之后告诉
                NodeManager为其启动Container。从深层次的含义讲YARN-Cluster和YARN-Client模式的区别其实就是ApplicationMaster进程的区别。
                YARN-Cluster模式下，Driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作
                业之后，就可以关掉Client，作业会继续在YARN上运行，因而YARN-Cluster模式不适合运行交互类型的作业。
                YARN-Client模式下，Application Master仅仅向YARN请求Executor，Client会和请求的Container通信来调度他们工作，也就是
                说Client不能离开。

7.SparkCore
    7.1 RDD
        RDD(Resilient Distributed Dateset)，弹性分布式数据集。
    7.2 RDD的五大特性
        1) RDD是由一系列的partition组成的。
        2) 函数是作用在每一个partition（split）上的。
        3) RDD之间有一系列的依赖关系。
        4) 分区器是作用在K,V格式的RDD上。
        5) RDD提供一系列最佳的计算位置。
    7.3 RDD理解图
        参照RDD示意图.png
        注意：
            textFile方法底层封装的是读取MR读取文件的方式，读取文件之前先split，默认split大小是一个block大小。
            RDD实际上不存储数据，这里方便理解，暂时理解为存储数据。
            K,V格式的RDD:如果RDD里面存储的数据都是二元组对象，那么这个RDD我们就叫做K,V格式的RDD。
            RDD的弹性（容错）:
                	partition数量，大小没有限制，体现了RDD的弹性
                	RDD之间依赖关系，可以基于上一个RDD重新计算出RDD。
            RDD的分布式: RDD是由Partition组成，partition是分布在不同节点上的。
            RDD提供计算最佳位置，体现了数据本地化。体现了大数据中“计算移动数据不移动”的理念。

    7.4 RDD运行流程
           RDD在Spark中运行大概分为以下三步：
               1) 创建RDD对象;
               2) DAGScheduler模块介入运算，计算RDD之间的依赖关系，RDD之间的依赖关系就形成了DAG;
               3) 每一个Job被分为多个Stage。划分Stage的一个主要依据是当前计算因子的输入是否是确定的，如果是则将其分在同一个Stage，避免多
               个Stage之间的消息传递开销。
           示例图参考RDD运行流程图.png
8.Spark代码流程
    8.1 创建SparkConf对象
           设置Application name。
        	设置运行模式及资源需求。
    8.2 创建SparkContext对象。
    8.3 基于Spark的上下文创建一个RDD，对RDD进行处理。
    8.4 应用程序中要有Action类算子来触发Transformation类算子执行。
    8.5 关闭Spark上下文对象SparkContext。
9.Transformations转换算子
    Transformations类算子是一类算子（函数）叫做转换算子，如map,flatMap,reduceByKey等。Transformations算子是延迟执行，也叫懒加载。
    Transformation类算子如下：
        	filter：过滤符合条件的记录数，true保留，false过滤掉。
        	map：将一个RDD中的每个数据项，通过map中的函数映射变为一个新的元素。特点：输入一条，输出一条数据。
        	mapPartitionWithIndex：类似于mapPartitions,除此之外还会携带分区的索引值。
        	repartition：增加或减少分区。会产生shuffle。（多个分区分到一个分区不会产生shuffle）
        	coalesce：coalesce常用来减少分区，第二个参数是减少分区的过程中是否产生shuffle。
                     true为产生shuffle，false不产生shuffle。默认是false。
                     如果coalesce设置的分区数比原来的RDD的分区数还多的话，第二个参数设置为false不会起作用，如果设置成true，
                     效果和repartition一样。即repartition(numPartitions) = coalesce(numPartitions,true)。
        	groupByKey：作用在K，V格式的RDD上。根据Key进行分组。作用在（K，V），返回（K，Iterable <V>）。
        	zip：将两个RDD中的元素（KV格式/非KV格式）变成一个KV格式的RDD,两个RDD的每个分区元素个数必须相同。
        	zipWithIndex：该函数将RDD中的元素和这个元素在RDD中的索引号（从0开始）组合成（K,V）对。
        	flatMap：先map后flat。与map类似，每个输入项可以映射为0到多个输出项。
        	sample：随机抽样算子，根据传进去的小数按比例进行又放回或者无放回的抽样。
        	reduceByKey：将相同的Key根据相应的逻辑进行处理。
        	sortByKey/sortBy：作用在K,V格式的RDD上，对key进行升序或者降序排序。
        	repartitionAndSortWithinPartitions：根据给定的分区程序对RDD进行重新分区，并在每个生成的分区内按键对记录进行排序。
            这比调用重新分区，然后在每个分区内进行排序更有效率，因为它可以将排序压入洗牌机器。
            主要用途：1）如果需要重分区，并且想要对分区中的数据进行升序排序。
                    2）提高性能，替换repartition和sortBy。
        	combineByKey:createCombiner,这个函数把当前的值作为参数，此时我们可以对其做些附加操作(类型转换)并把它返回 (这一步类似于初始化操作)
                         mergeValue,该函数把元素V合并到之前的元素C(createCombiner)上 (这个操作在每个分区内进行)
                         mergeCombiners,该函数把2个元素C合并 (这个操作在不同分区间进行)。
10.Action行动算子
    Action类算子也是一类算子（函数）叫做行动算子，如foreach,collect，count等。Transformations类算子是延迟执行，Action类算子是触发执行。
    一个application应用程序中有几个Action类算子执行，就有几个job运行。
    Action类算子如下：
        	count：返回数据集中的元素数。会在结果计算完成后回收到Driver端。
        	countByKey:作用到K,V格式的RDD上，根据Key计数相同Key的数据集元素。
        	countByValue:根据数据集每个元素相同的内容来计数。返回相同内容的元素对应的条数。
        	reduce:根据聚合逻辑聚合数据集中的每个元素。
        	take(n)：返回一个包含数据集前n个元素的集合。
        	first:first=take(1),返回数据集中的第一个元素。
        	foreach:循环遍历数据集中的每个元素，运行相应的逻辑。
        	collect：将计算结果回收到Driver端。
11.控制算子
    控制算子有三种，cache,persist,checkpoint，以上算子都可以将RDD持久化，持久化的单位是partition。cache和persist都是懒执行的。
    必须有一个action类算子触发执行。checkpoint算子不仅能将RDD持久化到磁盘，还能切断RDD之间的依赖关系。
    cache：默认将RDD的数据持久化到内存中。cache是懒执行。
    persist：可以指定持久化的级别。最常用的是MEMORY_ONLY和MEMORY_AND_DISK。”_2”表示有副本数。
        cache和persist的注意事项：
        1) cache和persist都是懒执行，必须有一个action类算子触发执行。
        2) cache和persist算子的返回值可以赋值给一个变量，在其他job中直接使用这个变量就是使用持久化的数据了。持久化的单位是partition。
        3) cache和persist算子后不能立即紧跟action算子。
            错误：rdd.cache().count() 返回的不是持久化的RDD，而是一个数值了。
    checkpoint:checkpoint将RDD持久化到磁盘，还可以切断RDD之间的依赖关系。
    checkpoint 的执行原理：
        1) 当RDD的job执行完毕后，会从finalRDD从后往前回溯。
        2) 当回溯到某一个RDD调用了checkpoint方法，会对当前的RDD做一个标记。
        3) Spark框架会自动启动一个新的job，重新计算这个RDD的数据，将数据持久化到HDFS上。
        优化：对RDD执行checkpoint之前，最好对这个RDD先执行cache，这样新启动的job只需要将内存中的数据拷贝到HDFS上就可以，省去了重新计算这一步。
12.Spark-Submit
    Options:
    	--master
     MASTER_URL, 可以是spark://host:port, mesos://host:port, yarn,  yarn-cluster,yarn-client, local
    	--deploy-mode
    DEPLOY_MODE, Driver程序运行的地方，client或者cluster,默认是client。
    	--class
    CLASS_NAME, 主类名称，含包名
    	--jars
    逗号分隔的本地JARS, Driver和executor依赖的第三方jar包
    	--files
    用逗号隔开的文件列表,会放置在每个executor工作目录中
    	--conf
    spark的配置属性
    	--driver-memory
    Driver程序使用内存大小（例如：1000M，5G），默认1024M
    	--executor-memory
    每个executor内存大小（如：1000M，2G），默认1G

    Spark standalone with cluster deploy mode only:
    	--driver-cores
    Driver程序的使用core个数（默认为1），仅限于Spark standalone模式
    Spark standalone or Mesos with cluster deploy mode only:
    	--supervise
    失败后是否重启Driver，仅限于Spark  alone或者Mesos模式
    Spark standalone and Mesos only:
    	--total-executor-cores
    executor使用的总核数，仅限于SparkStandalone、Spark on Mesos模式

    Spark standalone and YARN only:
    	--executor-cores
    每个executor使用的core数，Spark on Yarn默认为1，standalone默认为worker上所有可用的core。
    YARN-only:
    	--driver-cores
    driver使用的core,仅在cluster模式下，默认为1。
    	--queue
    QUEUE_NAME  指定资源队列的名称,默认：default
    	--num-executors
    一共启动的executor数量，默认是2个。
13.RDD Persistence（持久化）
    Storage Level（存储级别）	Meaning（含义）
    MEMORY_ONLY：将 RDD 以反序列化的 Java 对象的形式存储在 JVM 中. 如果内存空间不够，部分数据分区将不再缓存，在每次需要用到这些数据时重新进行计算. 这是默认的级别.
    MEMORY_AND_DISK：将 RDD 以反序列化的 Java 对象的形式存储在 JVM 中。如果内存空间不够，将未缓存的数据分区存储到磁盘，在需要使用这些分区时从磁盘读取.
    MEMORY_ONLY_SER：
    (Java and Scala)：将 RDD 以序列化的 Java 对象的形式进行存储（每个分区为一个 byte 数组）。这种方式会比反序列化对象的方式节省很多空间，尤其是在使用 fast serializer 时会节省更多的空间，但是在读取时会增加 CPU 的计算负担.
    MEMORY_AND_DISK_SER
    (Java and Scala):类似于 MEMORY_ONLY_SER ，但是溢出的分区会存储到磁盘，而不是在用到它们时重新计算.
    DISK_ONLY:只在磁盘上缓存 RDD.
    MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc:与上面的级别功能相同，只不过每个分区在集群中两个节点上建立副本.
    OFF_HEAP (experimental 实验性):类似于 MEMORY_ONLY_SER, 但是将数据存储在 off-heap memory 中. 这需要启用 off-heap 内存.
14.广播变量
    广播变量在每个节点上保存一个只读的变量的缓存, 而不用给每个 task 来传送一个 copy.
    例如, 给每个节点一个比较大的输入数据集是一个比较高效的方法. Spark 也会用该对象的广播逻辑去分发广播变量来降低通讯的成本.
    广播变量通过调用SparkContext.broadcast(v)来创建. 广播变量是对v的包装, 通过调用广播变量的 value方法可以访问.
    广播变量只会被发到各个节点一次，应作为只读值处理(修改广播变量的值不会影响到别的节点).
15.Spark Streaming
    Spark Streaming 是 Spark Core API 的扩展，它支持弹性的，高吞吐的，容错的实时数据流的处理。数据可以通过多种数据源获取，
    例如 Kafka，Flume，Kinesis 以及 TCP sockets，也可以通过例如 map，reduce，join，window 等的高级函数组成的复杂算法处理。
    最终，处理后的数据可以输出到文件系统，数据库以及实时仪表盘中。事实上，你还可以在 data streams（数据流）上使用机器学习以及图计算算法。
    在内部，它工作原理如Spark Streaming流程图.jpg所示，Spark Streaming 接收实时输入数据流并将数据切分成多个 batch（批）数据，
    然后由 Spark 引擎处理它们以生成最终的 stream of results in batches（分批流结果）。
    Spark Streaming 提供了一个名为 discretized stream 或 DStream 的高级抽象，它代表一个连续的数据流。DStream 可以从数据源
    的输入数据流创建，例如 Kafka，Flume 以及 Kinesis，或者在其他 DStream 上进行高层次的操作以创建。在内部，一个 DStream
    是通过一系列的 RDDs 来表示。
    StreamingContext 是所有流功能的主要入口点。
    1) 初始化 StreamingContext
        为了初始化一个 Spark Streaming 程序，一个 StreamingContext 对象必须要被创建出来，它是所有的 Spark Streaming 功能的主入口点,
        一个 StreamingContext 对象可以从一个 SparkConf 对象中来创建。
        在定义一个 context 之后,您必须执行以下操作。
        	通过创建输入 DStreams 来定义输入源。
        	通过应用转换和输出操作 DStreams 定义流计算（streaming computations）。
        	开始接收输入并且使用 streamingContext.start() 来处理数据。
        	使用 streamingContext.awaitTermination() 等待处理被终止（手动或者由于任何错误）。
        	使用 streamingContext.stop() 来手动的停止处理。
        需要注意的是：
        	一旦一个 context 已经启动，将不会有新的数据流的计算可以被创建或者添加到它。
        	一旦一个 context 已经停止，它不会被重新启动。
        	同一时间内在 JVM 中只有一个 StreamingContext 可以被激活。
        	在 StreamingContext 上的 stop() 同样也停止了 SparkContext。为了只停止 StreamingContext，
            设置 stop() 的可选参数，名叫 stopSparkContext 为 false。
        	一个 SparkContext 就可以被重用以创建多个 StreamingContexts，只要前一个 StreamingContext
            在下一个StreamingContext 被创建之前停止（不停止 SparkContext）。
    2） Discretized Streams (DStreams)（离散化流）
        Discretized Stream or DStream 是 Spark Streaming 提供的基本抽象。它代表了一个连续的数据流，无论是从 source（数据源）
        接收到的输入数据流，还是通过转换输入流所产生的处理过的数据流。在内部，一个 DStream 被表示为一系列连续的 RDDs，它是 Spark 中
        一个不可改变的抽象。在一个 DStream 中的每个 RDD 包含来自一定的时间间隔的数据，如DStream示意图.jpg所示。
    3) Input DStreams 和 Receivers（接收器）
        输入 DStreams 是代表输入数据是从流的源数据（streaming sources）接收到的流的 DStream。
        Spark Streaming 提供了两种内置的 streaming source（流的数据源）。
        	_Basic sources（基础的数据源）_：在 StreamingContext API 中直接可以使用的数据源。例如：file systems
            和 socket connections。
        	_Advanced sources（高级的数据源）_：像 Kafka，Flume，Kinesis，等等这样的数据源。可以通过额外的 utility classes
            来使用。像在 依赖 中讨论的一样，这些都需要额外的外部依赖。
        注意：
        	流处理程序中可以并行的接收多个数据流，然后创建多个 input DStreams。这将创建同时接收多个数据流的多个 receivers（接收器）。
            但需要注意，一个 Spark 的 worker/executor 是一个长期运行的任务（task），因此它将占用分配给 Spark Streaming 的应用程序的
            所有核中的一个核（core）。因此，要记住，一个 Spark Streaming 应用需要分配足够的核（core）（或线程（threads），如果本地运行
            的话）来处理所接收的数据，以及来运行接收器（receiver(s)）。
        	当在本地运行一个 Spark Streaming 程序的时候，不要使用 “local” 或者 “local[1]” 作为 master 的 URL。这两种方法中的任何一
            个都意味着只有一个线程将用于运行本地任务。如果你正在使用一个基于接收器（receiver）的输入离散流（input DStream）（例如，sockets，
            Kafka，Flume 等），则该单独的线程将用于运行接收器（receiver），而没有留下任何的线程用于处理接收到的数据。因此，在本地运行时，
            总是用 “local[n]” 作为 master URL，其中的 n > 运行接收器的数量（查看 Spark 属性 来了解怎样去设置 master 的信息）。
        	将逻辑扩展到集群上去运行，分配给 Spark Streaming 应用程序的内核（core）的内核数必须大于接收器（receiver）的数量。
            否则系统将接收数据，但是无法处理它。
    4) 基础的 Sources（数据源）
        File Streams: 用于从文件中读取数据，在任何与 HDFS API 兼容的文件系统中（即，HDFS，S3，NFS 等）,Spark Streaming 将监控
            dataDirectory 目录并且该目录中任何新建的文件 (写在嵌套目录中的文件是不支持的)。
            注意：
            	文件必须具有相同的数据格式。
            	文件必须被创建在 dataDirectory 目录中，通过 atomically（原子的）moving（移动） 或 renaming（重命名） 它们到数据目录。
            	一旦移动，这些文件必须不能再更改，因此如果文件被连续地追加，新的数据将不会被读取。
            	对于简单的文本文件，还有一个更加简单的方法 streamingContext.textFileStream(dataDirectory)。并且文件流（file streams）
                不需要运行一个接收器（receiver），因此，不需要分配内核（core）。
            	Python API 在 Python API 中 fileStream 是不可用的，只有 textFileStream 是可用的。
        Streams based on Custom Receivers（基于自定义的接收器的流）: DStreams 可以使用通过自定义的 receiver（接收器）接收到的数据来创建。
        ueue of RDDs as a Stream（RDDs 队列作为一个流）: 可以使用 streamingContext.queueStream(queueOfRDDs) 创建一个基于 RDDs
            队列的 DStream，每个进入队列的 RDD 都将被视为 DStream 中的一个批次数据，并且就像一个流进行处理。
    5) 高级 Sources（数据源）
        Kafka: Spark Streaming 2.2.0 与 Kafka broker 版本 0.8.2.1 或更高是兼容的。
        Flume: Spark Streaming 2.2.0 与 Flume 1.6.0 相兼容。
        Kinesis: Spark Streaming 2.2.0 与 Kinesis Client Library 1.2.1 相兼容。
    6) UpdateStateByKey 操作
        updateStateByKey 操作允许维护任意状态，同时不断更新新信息。需要通过两步来使用它。
        1.定义 state - state 可以是任何的数据类型。
        2.定义 state update function（状态更新函数）- 使用函数指定如何使用先前状态来更新状态，并从输入流中指定新值。
16.Spark SQL, DataFrames and Datasets
    1) Spark SQL: Spark 处理结构化数据的一个模块。与基础的 Spark RDD API 不同，Spark SQL 提供了查询结构化数据及计算结果等信息的接口。
    在内部，Spark SQL 使用这个额外的信息去执行额外的优化。有几种方式可以跟 Spark SQL 进行交互，包括 SQL 和 Dataset API。当使用相同执
    行引擎进行计算时，无论使用哪种 API / 语言都可以快速的计算。这种统一意味着开发人员能够在基于提供最自然的方式来表达一个给定的
    transformation API 之间实现轻松的来回切换不同的。
    Datasets and DataFrames:一个 Dataset 是一个分布式的数据集合 Dataset 是在 Spark 1.6 中被添加的新接口，它提供了 RDD 的优点
    （强类型化，能够使用强大的 lambda 函数）与Spark SQL执行引擎的优点。一个 Dataset 可以从 JVM 对象来构造并且使用转换功能
    （map，flatMap，filter，等等）
    Spark SQL中所有功能的入口点是 SparkSession 类。
    2) Spark on Hive:Hive只作为储存角色，Spark负责sql解析优化，执行。
       Hive on Spark：Hive即作为存储又负责sql的解析优化，Spark负责执行。
    3) DataFrame
        DataFrame也是一个分布式数据容器。与RDD类似，然而DataFrame更像传统数据库的二维表格，除了数据以外，还掌握数据的结构信息，即schema。
        同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从API易用性的角度上 看， DataFrame API提供的是一套高层
        的关系操作，比函数式的RDD API要更加友好，门槛更低。
        DataFrame的底层封装的是RDD，只不过RDD的泛型是Row类型。
    4)












